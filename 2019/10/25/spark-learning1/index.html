<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Spark工作机制（一） | 咖啡猫的网络日志</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="要了解Spark工作机制，首先要知道几个概念，第一个就是RDD： 1、什么是RDDRDD（Resilient Distributed Datasets） 是 Spark 的核心概念，中文名是弹性数据集，通俗的讲可以理解为是一种抽象的大规模数据集合，或者是一个大的数组，这个数组是分布在集群上的，Spark会在这个数据集合上做一系列的数据处理、计算，然后产生新的RDD，直到最后得到计算结果。 至于为什">
<meta name="keywords" content="spark,大数据,big data,rdd,工作机制">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark工作机制（一）">
<meta property="og:url" content="https:&#x2F;&#x2F;yyddbull.github.io&#x2F;2019&#x2F;10&#x2F;25&#x2F;spark-learning1&#x2F;index.html">
<meta property="og:site_name" content="咖啡猫的网络日志">
<meta property="og:description" content="要了解Spark工作机制，首先要知道几个概念，第一个就是RDD： 1、什么是RDDRDD（Resilient Distributed Datasets） 是 Spark 的核心概念，中文名是弹性数据集，通俗的讲可以理解为是一种抽象的大规模数据集合，或者是一个大的数组，这个数组是分布在集群上的，Spark会在这个数据集合上做一系列的数据处理、计算，然后产生新的RDD，直到最后得到计算结果。 至于为什">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https:&#x2F;&#x2F;yyddbull.github.io&#x2F;coffeecat&#x2F;2019&#x2F;10&#x2F;25&#x2F;spark-learning1&#x2F;rdd_deploy.png">
<meta property="og:updated_time" content="2019-10-29T02:51:42.922Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;yyddbull.github.io&#x2F;coffeecat&#x2F;2019&#x2F;10&#x2F;25&#x2F;spark-learning1&#x2F;rdd_deploy.png">
  
    <link rel="alternate" href="/coffeecat/atom.xml" title="咖啡猫的网络日志" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/coffeecat/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/coffeecat/" id="logo">咖啡猫的网络日志</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/coffeecat/">首页</a>
        
          <a class="main-nav-link" href="/coffeecat/archives">归档</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/coffeecat/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://yyddbull.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-spark-learning1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/coffeecat/2019/10/25/spark-learning1/" class="article-date">
  <time datetime="2019-10-25T05:52:09.000Z" itemprop="datePublished">2019-10-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/coffeecat/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Spark工作机制（一）
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>要了解Spark工作机制，首先要知道几个概念，第一个就是RDD：</p>
<h1 id="1、什么是RDD"><a href="#1、什么是RDD" class="headerlink" title="1、什么是RDD"></a>1、什么是RDD</h1><p>RDD（Resilient Distributed Datasets） 是 Spark 的核心概念，中文名是弹性数据集，通俗的讲可以理解为是一种抽象的大规模数据集合，或者是一个大的数组，这个数组是分布在集群上的，Spark会在这个数据集合上做一系列的数据处理、计算，然后产生新的RDD，直到最后得到计算结果。</p>
<p>至于为什么叫弹性数据集，弹性如何解释？这里是指在任何时候都能进行重算。举个例子，当集群中的一台节点故障导致RDD丢失后，Spark还可以重新计算出这部分的分区的数据，所以，RDD是一种天生具有容错机制的特殊集合，不需要通过数据冗余的方式（比如检查点）实现容错，对用户来说，感觉不到这部分的内容曾经丢失，所以RDD数据集就像一个海绵一样，无论如何挤压都是完整的。</p>
<h1 id="2、RDD的特性"><a href="#2、RDD的特性" class="headerlink" title="2、RDD的特性"></a><strong>2、RDD的特性</strong></h1><p>Spark官网对RDD的描述：</p>
<ul>
<li>A list of partitions</li>
<li>A function for computing each split</li>
<li>A list of dependencies on other RDDs</li>
<li>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)</li>
<li>Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)</li>
</ul>
<p>解释如下：</p>
<ul>
<li>是一组分区（partition），类似hadoop，能够被切分，从而实现并行计算，如图1所示，RDD1中有3个区分（p1,p3,p4），分别存储在3个节点上</li>
<li>有一个函数计算分片，每个RDD都会实现compute函数,对每个分区内的数据进行计算</li>
<li>依赖其他RDD的列表，例如由于RDD的转换生成一个新的RDD，这样RDD之间就会形成类似于流水线一样的前后依赖关系</li>
<li>可选，一个分区函数。Spark中有HashPartitioner和RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None</li>
<li>可选，一个存储存取每个Partition的优先位置的列表。对于HDFS文件来说，这个列表保存的就是每个Partition所在块的位置</li>
</ul>
<p><img src="/coffeecat/2019/10/25/spark-learning1/rdd_deploy.png" alt></p>
<h1 id="3、RDD的操作分类"><a href="#3、RDD的操作分类" class="headerlink" title="3、RDD的操作分类"></a><strong>3、RDD的操作分类</strong></h1><p> RDD有2种操作类型： </p>
<ul>
<li>转换(transformations) ：从已经存在的数据集中创建一个新的数据集，会创建一个新的RDD，例如map操作，会针对将数据集的每个元素传给函数处理，并生成一个新的RDD</li>
<li>动作(actions) ：在数据集上进行计算之后返回一个值到驱动程序，例如reduce动作，使用函数聚合RDD所有元素，并将结果返回给驱动程序</li>
</ul>
<p>常用的Transformation如下：</p>
<ul>
<li>map(func)：返回一个新的分布式数据集，该数据集由每一个输入元素经过func函数转换后组成</li>
<li>fitler(func)：返回一个新的数据集，该数据集由经过func函数计算后返回值为true的输入元素组成</li>
<li>flatMap(func)：类似于map，但是每一个输入元素可以被映射为0或多个输出元素（因此func返回一个序列，而不是单一元素）</li>
<li>mapPartitions(func)：类似于map，但独立地在RDD上每一个分片上运行，因此在类型为T的RDD上运行时，func函数类型必须是Iterator[T]=&gt;Iterator[U]</li>
<li>mapPartitionsWithSplit(func)：类似于mapPartitons，但func带有一个整数参数表示分片的索引值。因此在类型为T的RDD上运行时，func函数类型必须是(Int,Iterator[T])=&gt;Iterator[U]</li>
<li>sample(withReplacement,fraction,seed)：根据fraction指定的比例对数据进行采样，可以选择是否用随机数进行替换，seed用于随机数生成器种子</li>
<li>union(otherDataSet)：返回一个新数据集，新数据集是由原数据集和参数数据集联合而成</li>
<li>distinct([numTasks])：返回一个包含原数据集中所有不重复元素的新数据集</li>
<li>groupByKey([numTasks])：在一个(K,V)数据集上调用，返回一个(K,Seq[V])对的数据集。注意默认情况下，只有8个并行任务来操作，但是可以传入一个可选的numTasks参数来改变它</li>
<li>reduceByKey(func,[numTasks])：在一个(K,V)对的数据集上调用，返回一个(K,V)对的数据集，使用指定的reduce函数，将相同的key的值聚合到一起。与groupByKey类似，reduceByKey任务的个数是可以通过第二个可选参数来设置的</li>
<li>sortByKey([[ascending],numTasks])：在一个(K,V)对的数据集上调用，K必须实现Ordered接口，返回一个按照Key进行排序的(K,V)对数据集。升序或降序由ascending布尔参数决定</li>
<li>join(otherDataset0,[numTasks])：在类型为(K,V)和(K,W)数据集上调用，返回一个相同的key对应的所有元素在一起的(K,(V,W))数据集</li>
<li>cogroup(otherDataset,[numTasks])：在类型为(K,V)和(K,W)数据集上调用，返回一个(K,Seq[V],Seq[W])元祖的数据集。这个操作也可以称为groupwith</li>
<li>cartesain(ohterDataset)：笛卡尔积，在类型为T和U类型的数据集上调用，返回一个(T,U)对数据集(两两的元素对)</li>
</ul>
<p>常用的Action如下：</p>
<ul>
<li>reduce(func)：通过函数func(接收两个参数，返回一个参数)聚集数据集中的所有元素。这个功能必须可交换且可关联的，从而可以正确的并行运行</li>
<li>collect()：在驱动程序中，以数组形式返回数据集中的所有元素。通常在使用filter或者其他操作返回一个足够小的数据子集后再使用会比较有用</li>
<li>count()：返回数据集元素个数</li>
<li>first()：返回数据集第一个元素(类似于take(1))</li>
<li>take(n)：返回一个由数据集前n个元素组成的数组，注意 这个操作目前并非并行执行，而是由驱动程序计算所有的元素</li>
<li>takeSample(withReplacement,num,seed)：返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否由随机数替换不足的部分，seed用户指定随机数生成器种子</li>
<li>saveAsTextFile(path)：将数据集的元素以textfile的形式保存到本地文件系统–HDFS或者任何其他Hadoop支持的文件系统。对于每个元素，Spark将会调用toString方法，将它转换为文件中的文本行</li>
<li>saveAsSequenceFile(path)：将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以是本地系统、HDFS或者任何其他的Hadoop支持的文件系统。这个只限于由key-value对组成，并实现了Hadoop的Writable接口，或者可以隐式的转换为Writable的RDD(Spark包括了基本类型转换，例如Int、Double、String等)</li>
<li>countByKey()：对(K,V)类型的RDD有效，返回一个(K,Int)对的map，表示每一个key对应的元素个数</li>
<li>foreach(func)：在数据集的每一个元素上，运行函数func进行更新。通常用于边缘效果，例如更新一个叠加器，或者和外部存储系统进行交互，如HBase</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://yyddbull.github.io/2019/10/25/spark-learning1/" data-id="ck2czn5ro0002pj061wutgxk9" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/coffeecat/tags/spark-%E5%A4%A7%E6%95%B0%E6%8D%AE-big-data-rdd-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/" rel="tag">spark,大数据,big data,rdd,工作机制</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/coffeecat/2019/10/27/spark-learning2/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Spark工作机制（二）
        
      </div>
    </a>
  
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/coffeecat/categories/spark/">spark</a></li><li class="category-list-item"><a class="category-list-link" href="/coffeecat/categories/%E5%85%B6%E4%BB%96/">其他</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/coffeecat/tags/spark-driver-%E6%83%B0%E6%80%A7%E8%AE%A1%E7%AE%97-%E6%89%A7%E8%A1%8C%E6%9C%BA%E5%88%B6-spark-application/" rel="tag">spark,driver,惰性计算,执行机制,spark application</a></li><li class="tag-list-item"><a class="tag-list-link" href="/coffeecat/tags/spark-%E5%A4%A7%E6%95%B0%E6%8D%AE-big-data-rdd-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/" rel="tag">spark,大数据,big data,rdd,工作机制</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/coffeecat/tags/spark-driver-%E6%83%B0%E6%80%A7%E8%AE%A1%E7%AE%97-%E6%89%A7%E8%A1%8C%E6%9C%BA%E5%88%B6-spark-application/" style="font-size: 10px;">spark,driver,惰性计算,执行机制,spark application</a> <a href="/coffeecat/tags/spark-%E5%A4%A7%E6%95%B0%E6%8D%AE-big-data-rdd-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/" style="font-size: 10px;">spark,大数据,big data,rdd,工作机制</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/coffeecat/archives/2019/10/">十月 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/coffeecat/2019/10/29/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/coffeecat/2019/10/27/spark-learning2/">Spark工作机制（二）</a>
          </li>
        
          <li>
            <a href="/coffeecat/2019/10/25/spark-learning1/">Spark工作机制（一）</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 咖啡猫<br>
      Powered by <a href="http://hexo.io/" target="_blank">coffeecat</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/coffeecat/" class="mobile-nav-link">首页</a>
  
    <a href="/coffeecat/archives" class="mobile-nav-link">归档</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/coffeecat/fancybox/jquery.fancybox.css">
  <script src="/coffeecat/fancybox/jquery.fancybox.pack.js"></script>


<script src="/coffeecat/js/script.js"></script>



  </div>
</body>
</html>